{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Language Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we import the necessary libraries for building and working with a neural language model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from random import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load the data that we will be using to build the neural language model and preprocess it by tokenizing and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data into a Pandas dataframe\n",
    "data_path = 'datasets/data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocess data\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Language Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we define a class for the neural language model using PyTorch. This class includes an embedding layer, a LSTM layer, and a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        x, h = self.lstm(x, h)\n",
    "        x = x.contiguous().view(-1, x.shape[2])\n",
    "        x = self.fc(x)\n",
    "        return x, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we define a function to train the neural language model using the preprocessed data. This function loops over the data and performs backpropagation on each batch to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, epochs, batch_size, sequence_length, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        h = (torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(device),\n",
    "             torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size).to(device))\n",
    "        for i in range(0, len(data)-sequence_length, sequence_length):\n",
    "            # Extract the sequence from the data\n",
    "            x = data[i:i+sequence_length]\n",
    "            y = data[i+1:i+sequence_length+1]\n",
    "\n",
    "            # Convert sequences to torch tensors\n",
    "            try:\n",
    "                # Assuming x and y are lists of integers\n",
    "                x = torch.tensor(x, dtype=torch.long).view(sequence_length, -1).to(device)\n",
    "                y = torch.tensor(y, dtype=torch.long).view(-1).to(device)\n",
    "            except ValueError as e:\n",
    "                # Handle the case where x and y are not lists of integers\n",
    "                print(f\"ValueError: {e}. Check that your data only contains integers.\")\n",
    "                return\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, backward pass, and optimize\n",
    "            output, h = model(x, h)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "        \n",
    "        # print(f'Epoch {epoch+1}/{epochs} loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    \"\"\"\n",
    "    Saves the trained model to disk.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained model to save.\n",
    "    model_path (str): The path to the file where the model should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure the model_path is a string and not empty\n",
    "    if not isinstance(model_path, str) or not model_path:\n",
    "        raise ValueError(\"Please provide a valid model path as a string.\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    try:\n",
    "        # If using a GPU, move the model to CPU to avoid unnecessary GPU RAM usage\n",
    "        # during loading if the user doesn't have a GPU setup.\n",
    "        model_to_save = model.cpu()\n",
    "        torch.save(model_to_save.state_dict(), model_path)\n",
    "        print(f\"Model successfully saved to {model_path}\")\n",
    "    except Exception as e:\n",
    "        # Handle exceptions that may occur during the save process.\n",
    "        print(f\"An error occurred while saving the model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we define the hyperparameters for the neural language model, initialize the model, define the loss and optimizer, and train the model using the train function we defined earlier. We then save the trained model to disk using the save_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to model.pt\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(set([word for tokens in data['tokens'] for word in tokens]))\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "sequence_length = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, data['tokens'], optimizer, criterion, epochs, batch_size, sequence_length, device)\n",
    "\n",
    "# Save model\n",
    "model_path = 'model.pt'\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethos",
   "language": "python",
   "name": "ethos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
