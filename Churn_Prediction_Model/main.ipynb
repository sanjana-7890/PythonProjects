{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>SpeakX Assignment</u>\n",
    "#### Predicting Customer Churn in a Telecommunication Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries.\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the installed libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Collection and Loading\n",
    "\n",
    "The data is loaded from the given Kaggle Link <u><a href = \"https://www.kaggle.com/datasets/blastchar/telcocustomer-churn\"> https://www.kaggle.com/datasets/blastchar/telcocustomer-churn</a></u> so that I can proceed with the further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored from .csv file in pandas dataframe.\n",
    "data= pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information regarding the dataset.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics of the dataset.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "• Converted the wrong type of numerical column to numeric.<br>\n",
    "• Dropped the unwanted and useless columns.<br>\n",
    "• Encoded the categorical columns to numeric to make fir for models.<br>\n",
    "\n",
    "<u>Note</u>: After encoding multiple categories column,the number of column increased from 21 to 41."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted 'TotalCharges' to numeric.\n",
    "data['TotalCharges']= pd.to_numeric(data['TotalCharges'],errors='coerce')\n",
    "\n",
    "# Handled missing values by replacing with the mean of the column.\n",
    "data['TotalCharges'].fillna(data['TotalCharges'].mean(), inplace=True)\n",
    "\n",
    "# Dropped the 'customerID' column.\n",
    "data.drop('customerID',axis=1,inplace=True)\n",
    "\n",
    "# Encoded binary categorical variables.\n",
    "binary_columns = ['gender','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\n",
    "for col in binary_columns:\n",
    "    data[col] = data[col].apply(lambda x: 1 if x == 'Yes' or x == 'Male' else 0)\n",
    "\n",
    "# Encoded non-binary categorical variables using one-hot encoding.\n",
    "dataset = pd.get_dummies(data)\n",
    "\n",
    "# Displayed the first few rows of the preprocessed dataset.\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of newly created boolean columns.\n",
    "boolean_columns = ['MultipleLines_No', 'MultipleLines_No phone service', 'MultipleLines_Yes',\n",
    "                   'InternetService_DSL', 'InternetService_Fiber optic', 'InternetService_No',\n",
    "                   'OnlineSecurity_No', 'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n",
    "                   'OnlineBackup_No', 'OnlineBackup_No internet service', 'OnlineBackup_Yes',\n",
    "                   'DeviceProtection_No', 'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n",
    "                   'TechSupport_No', 'TechSupport_No internet service', 'TechSupport_Yes',\n",
    "                   'StreamingTV_No', 'StreamingTV_No internet service', 'StreamingTV_Yes',\n",
    "                   'StreamingMovies_No', 'StreamingMovies_No internet service', 'StreamingMovies_Yes',\n",
    "                   'Contract_Month-to-month', 'Contract_One year', 'Contract_Two year',\n",
    "                   'PaymentMethod_Bank transfer (automatic)', 'PaymentMethod_Credit card (automatic)',\n",
    "                   'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']\n",
    "\n",
    "# Converted boolean columns to integers.\n",
    "dataset[boolean_columns] = dataset[boolean_columns].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the updated dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is performed to gain insights from the dataset initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The number of 'Churned' and 'Not Churned' customers are identified from the dataset. <br>\n",
    "• It's clearly visible that the dataset is imbalanced as the number of 'Not Churned' customers are high (approx. 75%) due to which 'Churned' customers characteristics are less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['Churn'].value_counts())\n",
    "\n",
    "# Distribution of churned vs non-churned customers.\n",
    "sns.countplot(x='Churn', data=dataset)\n",
    "plt.title('Distribution of Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In univariate analysis, 3 numeric columns were analysed.\n",
    " \n",
    "<u>Observation</u><br>\n",
    "\n",
    "• The distribution for tenure is close to multimodal. It could indicates that customers have varying lengths of association with the telecom company.This could imply diverse customer segments with different levels of loyalty.<br>\n",
    "\n",
    "• The distribution of monthly charges,after the initial bars,approximates a normal distribution pattern.This indicates that most customers are charged around a central average amount.<br>\n",
    "\n",
    "• The  distribution of total charges is positively skewed highlights that while most customers incur lower charges,a smaller group accounts for higher charges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis: Histogram of tenure,monthly charges and total charges is plotted.\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(dataset['tenure'], bins=20, kde=True)\n",
    "plt.title('Distribution of Tenure')\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(dataset['MonthlyCharges'], bins=20, kde=True)\n",
    "plt.title('Distribution of Monthly Charges')\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(dataset['TotalCharges'], bins=20, kde=True)\n",
    "plt.title('Distribution of Total Charges')\n",
    "plt.xlabel('Total Charges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bivariate analysis, 3 numeric columns were analysed.\n",
    " \n",
    "<u>Observation</u><br>\n",
    "\n",
    "• The boxplot for tenure identifies certain small outliers which are greater than the upper fence. Also the median tenure for 'Churned' customers is very less depicting they are much likely to churn.<br>\n",
    "\n",
    "• The median monthly charges as well as minimum value is less for 'Not Churned' customers depicting they are less likely to churn.<br>\n",
    "\n",
    "• The total charges column is showing outliers mostly due to the large scale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: Box plot of tenure,monthly_charges and total_charges by Churn.\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Churn', y='tenure', data=dataset)\n",
    "plt.title('Tenure by Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Tenure')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Churn', y='MonthlyCharges', data=dataset)\n",
    "plt.title('Monthly Charges by Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Churn', y='TotalCharges', data=dataset)\n",
    "plt.title('Total Charges by Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Total Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• In multivariate analysis, all columns were analysed using heatmap which is nothing but visual version of correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of correlation matrix.\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(dataset.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Many other insights were drawn from different graphs</u> - \n",
    "\n",
    "• Gender-wise proportion of customers is almost same for the company.<br>\n",
    "• Female customers churned more than men but on very slight margin.<br>\n",
    "• Company's customer domain include only around 15% as senior citizens and in which nearly 50% churned.<br>\n",
    "• Around 75% of non-senior citizens stick with the company.<br>\n",
    "• The number of such customers are more who are living as partners.<br>\n",
    "• Among customers who were living with their partners, 70% were happy with the services of company which is more in comparison to the customers not living with partners.<br>\n",
    "• There are only around 30% of customers who have dependants and out of those nearly 82% stick to the services provide by the telecom company.<br>\n",
    "• Around 90% of the customers have opted for the phone service and out of those 25% were at the risk of churning.<br>\n",
    "• At the same time around 90% percent of customers are retaining with the company if they haven't opted for phone service.<br>\n",
    "• The churn count is more in customers who have access to paperless billing than compared to those who have not. The main reason could be that paperless billing slightly imposes that the customer is more more advanced and might be using internet more where he/she could be available to other rival telecom company services.<br>\n",
    "• Churning rate is very less in customers whose payment method is set to automatic either by bank or by credit card.<br>\n",
    "• Churning rate is observed high in customers with month-to-month subscription that to 1 year or 2 year subscription.<br><br>\n",
    "\n",
    "<i>Note : There could more visualizations that could be extracted but I am limiting myself to here.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of binary categorical features.\n",
    "categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "\n",
    "# Plotted distribution and relationship with churn\n",
    "for feature in categorical_features:\n",
    "    # Calculate churn counts by category\n",
    "    churn_counts = dataset.groupby([feature, 'Churn']).size().unstack()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    churn_counts.plot(kind='bar', stacked=True, color=['green', 'red'], alpha=0.7)\n",
    "    plt.title(f'Churn Counts by {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Churn', labels=['Not Churn', 'Churn'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate churn rate by contract type\n",
    "contract_churn_rate = data.groupby('Contract')['Churn'].mean() * 100\n",
    "\n",
    "# Plot churn rate by contract type\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=contract_churn_rate.index, y=contract_churn_rate.values, color='blue')\n",
    "plt.title('Churn Rate by Contract Type')\n",
    "plt.xlabel('Contract Type')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate churn rate by contract type\n",
    "contract_churn_rate = data.groupby('PaymentMethod')['Churn'].mean() * 100\n",
    "\n",
    "# Plot churn rate by contract type\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=contract_churn_rate.index, y=contract_churn_rate.values, color='blue')\n",
    "plt.title('Churn Rate by Payment Method')\n",
    "plt.xlabel('Payment Method Type')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am planning to add 2 more feature as it will give more light namely - \n",
    "\n",
    "• Total_Services - It will reflect the number of services opted by the customers which suggests that if customer opter for as many services then it means they are happy with us and less chance for churning.<br>\n",
    "• Monthly_Charges_per_Service - It reflects the average service charge that the customer is paying to the company on a monthly basis. If its less then mostly the churning is decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns representing each service.\n",
    "yes_service_columns = [\n",
    "    'PhoneService', \n",
    "    'MultipleLines_Yes', \n",
    "    'InternetService_DSL', \n",
    "    'InternetService_Fiber optic', \n",
    "    'OnlineSecurity_Yes', \n",
    "    'OnlineBackup_Yes', \n",
    "    'DeviceProtection_Yes', \n",
    "    'TechSupport_Yes', \n",
    "    'StreamingTV_Yes', \n",
    "    'StreamingMovies_Yes'\n",
    "]\n",
    "\n",
    "# Created a new feature representing the total number of services subscribed to by each customer\n",
    "dataset['TotalServices'] = dataset[yes_service_columns].sum(axis=1)\n",
    "\n",
    "# Calculated the average monthly charge per service for each customer\n",
    "dataset['MonthlyChargesPerService'] = dataset.apply(\n",
    "    lambda row: row['MonthlyCharges'] / row['TotalServices'] if row['TotalServices'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(dataset[['TotalServices', 'MonthlyChargesPerService']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the exact figure which gives somewhat light - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_with_2_or_more_services = dataset[dataset['TotalServices'] >= 2]\n",
    "\n",
    "# Count the number of customers who churned after availing 2 or more services\n",
    "num_churned_after_2_or_more_services = customers_with_2_or_more_services[customers_with_2_or_more_services['Churn'] == 1].shape[0]\n",
    "\n",
    "print(\"Number of customers who churned after availing 2 or more services:\", num_churned_after_2_or_more_services)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training and Evaluation\n",
    "\n",
    "I would be using 3 models namely Logistic regression, random forest classifier and gradient boosting algorithm as all theses 3 are primarily used in classification problems and specially binary classification like Churn(1) or Not Churn(0).\n",
    "\n",
    "Later I would perform a separate random forest classifier using hyperparatmeter tuning (GridSearchCV is the most common way for tuning the hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necesary libraries are already imported.\n",
    "# Defined the feature and the target variable.\n",
    "X = dataset.drop(columns=['Churn'])\n",
    "y = dataset['Churn']\n",
    "\n",
    "# Splitted the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Then fitted the scaler on the training set and transform both the training and testing set.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Then initialized the logistic regression model.\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Training themodel.\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making prediction with the model.\n",
    "log_reg_y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Finally evaluated the model performance.\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_y_pred)\n",
    "log_reg_precision = precision_score(y_test, log_reg_y_pred)\n",
    "log_reg_recall = recall_score(y_test, log_reg_y_pred)\n",
    "log_reg_f1 = f1_score(y_test, log_reg_y_pred)\n",
    "\n",
    "print(\"Logistic Regression- Accuracy:\", log_reg_accuracy)\n",
    "print(\"Logistic Regression- Precision:\", log_reg_precision)\n",
    "print(\"Logistic Regression- Recall:\", log_reg_recall)\n",
    "print(\"Logistic Regression- F1 Score:\", log_reg_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• <u>Strength</u>: Logistic regression shows the highest accuracy and F1 score among the models, indicating a balanced performance in terms of precision and recall.<br>\n",
    "• <u>Weakness</u>: The recall is lower compared to precision suggesting that the model could miss some churned customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necesary libraries are already imported.\n",
    "# Defined the feature and the target variable.\n",
    "X = dataset.drop(columns=['Churn'])\n",
    "y = dataset['Churn']\n",
    "\n",
    "# Splitted the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialized the scaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitted the scaler on the training set and transform both the training and testing sets.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Then initialize the gradient boosting model.\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Trained the model.\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Made predictions with the  model.\n",
    "gb_y_pred = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# At last evvaluated the gradient boosting model.\n",
    "gb_accuracy = accuracy_score(y_test, gb_y_pred)\n",
    "gb_precision = precision_score(y_test, gb_y_pred)\n",
    "gb_recall = recall_score(y_test, gb_y_pred)\n",
    "gb_f1 = f1_score(y_test, gb_y_pred)\n",
    "\n",
    "print(\"Gradient Boosting - Accuracy:\", gb_accuracy)\n",
    "print(\"Gradient Boosting - Precision:\", gb_precision)\n",
    "print(\"Gradient Boosting - Recall:\", gb_recall)\n",
    "print(\"Gradient Boosting - F1 Score:\", gb_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• <u>Strength</u>: Gradient boosting offers a good balance between precision and recall, though slightly lower than logistic regression in overall metrics.<br>\n",
    "• <u>Weakness</u>: The recall is slightly lower than precision, indicating some room for improvement in identifying churned customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necesary libraries are already imported.\n",
    "# Define the feature and the target variable.\n",
    "X = dataset.drop(columns=['Churn'])\n",
    "y = dataset['Churn']\n",
    "\n",
    "# Splitted the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialized the scaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitteed the scaler on the training set and transform both the training and testing sets.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Then initialized the random forest model.\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Trained the random forest model.\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions with the model.\n",
    "rf_y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Finally evaluated the performance of model.\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_precision = precision_score(y_test, rf_y_pred)\n",
    "rf_recall = recall_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred)\n",
    "\n",
    "print(\"Random Forest - Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest - Precision:\", rf_precision)\n",
    "print(\"Random Forest - Recall:\", rf_recall)\n",
    "print(\"Random Forest - F1 Score:\", rf_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• <u>Strength</u>: Random forest provides a robust model with decent precision but suffers from a lower recall leading to a lower F1 score.<br>\n",
    "• <u>Weakness</u>: Recall is the weakest among the three modelsmeaning it misses large number of churned customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4. Random Forest Classifier (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the feature and the target variable.\n",
    "X = dataset.drop(columns=['Churn'])\n",
    "y = dataset['Churn']\n",
    "\n",
    "# Splitted the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defined the parameter grid for hyperparameter tuning.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [None, 10, 20],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "# Then initialized Random Forest classifier.\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Also initialized grid search cross-validation.\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Performed grid search cross-validation.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Then got the best hyperparameters.\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Made predictions with the best model.\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model_y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluated the best model performance.\n",
    "best_model_accuracy = accuracy_score(y_test, best_model_y_pred)\n",
    "best_model_precision = precision_score(y_test, best_model_y_pred)\n",
    "best_model_recall = recall_score(y_test, best_model_y_pred)\n",
    "best_model_f1 = f1_score(y_test, best_model_y_pred)\n",
    "\n",
    "print(\"Best Model - Accuracy:\", best_model_accuracy)\n",
    "print(\"Best Model - Precision:\", best_model_precision)\n",
    "print(\"Best Model - Recall:\", best_model_recall)\n",
    "print(\"Best Model - F1 Score:\", best_model_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• <u>Strength</u>: The tuned random forest shows a more balanced performance and surpasses the base random forest model.<br>\n",
    "• <u>Weakness</u>: Even after improvement, recall remains a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i> Final Observation</i></u>\n",
    "\n",
    "• Logistic regression emerges as the best performer in terms of accuracy and F1 score, making it the most reliable model for churn prediction in this analysis. However, there is a trade-off between precision and recall, indicating the potential need for further optimization or combining models to improve recall rates.<br>\n",
    "\n",
    "• The gradient boosting and tuned random forest models are also good alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= \"red\">************************************************************COMPLETED************************************************************</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
